# -*- coding: utf-8 -*-
"""RD2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e-IFUTqRgsxztt5tKvWIHXmlUNl7c6JX
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import AUC


csv_file_path = '/content/ad.csv'


data = pd.read_csv(csv_file_path)

data.head(10)

import pandas as pd
ad_counts = data['Ad Topic Line'].value_counts()
print(ad_counts.head(10))
city_counts = data['City'].value_counts()
print(city_counts.head(10))
country_counts = data['Country'].value_counts()
print(country_counts.head(10))


columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)


data['Timestamp'] = pd.to_datetime(data['Timestamp'])

data['Year'] = data['Timestamp'].dt.year
data['Month'] = data['Timestamp'].dt.month
data['Day'] = data['Timestamp'].dt.day
data['Hour'] = data['Timestamp'].dt.hour
data['Minute'] = data['Timestamp'].dt.minute
data['Second'] = data['Timestamp'].dt.second
data['DayOfWeek'] = data['Timestamp'].dt.dayofweek

data['Age_Gender_Interact'] = data['Age'].astype(str) + '_' + data['Gender']

data.head(10)

y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)

categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)
scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])

n_components = 250
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_processed)


import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import pandas as pd
from deepctr.layers import FM



# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X_pca, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define the DNN component
def create_dnn(input_dim):
    inputs = tf.keras.layers.Input(shape=(input_dim,))
    dense1 = tf.keras.layers.Dense(units=256, activation='relu')(inputs)
    dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dense1)
    dense3 = tf.keras.layers.Dense(units=64, activation='relu')(dense2)
    output_dnn = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense3)
    return tf.keras.models.Model(inputs=inputs, outputs=output_dnn)

# Create and compile the DNN model
input_dim = X_train.shape[1]
dnn_model = create_dnn(input_dim)

dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the FM-NN component
def create_fm_nn(input_dim):
    input_layer = tf.keras.layers.Input(shape=(input_dim,))
    fm_out = FM()(input_layer)

    def create_nn(input_dim):
        inputs = tf.keras.layers.Input(shape=(input_dim,))
        dense1 = tf.keras.layers.Dense(units=256, activation='relu')(inputs)
        dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dense1)
        dense3 = tf.keras.layers.Dense(units=64, activation='relu')(dense2)
        output_nn = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense3)
        return tf.keras.models.Model(inputs=inputs, outputs=output_nn)

    # Create and compile the NN model
    nn_model = create_nn(input_dim)
    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Combine FM and NN outputs
    combined_output = tf.keras.layers.concatenate([fm_out, nn_model.output])

    # Fusion layer
    fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)

    # Output layer
    output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)

    # Create the composite FM-NN model
    fm_nn_model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)

    return fm_nn_model

# Create the composite model
input_dim = X_train.shape[1]
composite_model = create_fm_nn(input_dim)

# Compile the composite model
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the composite model
composite_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)

# Evaluate the composite model on the test set
loss, accuracy = composite_model.evaluate(X_test, y_test)
print(f"Test Loss (Composite): {loss:.4f}, Test Accuracy (Composite): {accuracy:.4f}")

# Predict and evaluate
y_pred_composite = composite_model.predict(X_test)
auc_score_composite = roc_auc_score(y_test, y_pred_composite)
logloss_composite = log_loss(y_test, y_pred_composite)

print(f"AUC Score (Composite): {auc_score_composite:.4f}")
print(f"Log Loss (Composite): {logloss_composite:.4f}")



# X_train, X_temp, y_train, y_temp = train_test_split(X_pca, y, test_size=0.3, random_state=42)
# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# X_train, X_temp, y_train, y_temp = train_test_split(X_pca, y, test_size=0.3, random_state=42)
# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# def create_dnn(input_dim):
#     inputs = tf.keras.layers.Input(shape=(input_dim,))
#     dense1 = tf.keras.layers.Dense(units=256, activation='relu')(inputs)
#     dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dense1)
#     dense3 = tf.keras.layers.Dense(units=64, activation='relu')(dense2)
#     output_dnn = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense3)
#     model_dnn = tf.keras.models.Model(inputs=inputs, outputs=output_dnn)
#     return model_dnn

# def create_fnn(input_dim):
#     inputs = tf.keras.layers.Input(shape=(input_dim,))
#     dense1 = tf.keras.layers.Dense(units=32, activation='relu')(inputs)
#     for _ in range(9):
#         dense1 = tf.keras.layers.Dense(units=32, activation='relu')(dense1)
#     output_fnn = dense1
#     model_fnn = tf.keras.models.Model(inputs=inputs, outputs=output_fnn)
#     return model_fnn


# input_dim = X_train.shape[1]
# dnn_model = create_dnn(input_dim)
# fnn_model = create_fnn(input_dim)


# combined_output = tf.keras.layers.concatenate([dnn_model.output, fnn_model.output])

# fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)


# output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)


# composite_model = tf.keras.models.Model(inputs=[dnn_model.input, fnn_model.input], outputs=output_layer)


# composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# composite_model.fit([X_train, X_train], y_train, epochs=5, batch_size=32, validation_split=0.2)


# loss, accuracy = composite_model.evaluate([X_test, X_test], y_test)
# print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")


# y_pred = composite_model.predict([X_test, X_test])
# auc_score = roc_auc_score(y_test, y_pred)
# logloss = log_loss(y_test, y_pred)

# print(f"AUC Score: {auc_score:.4f}")
# print(f"Log Loss: {logloss:.4f}")

!pip install deepctr

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from deepctr.layers import FM
import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss
from deepctr.layers import FM

# Load the dataset
csv_file_path = '/content/ad.csv'
data = pd.read_csv(csv_file_path)

ad_counts = data['Ad Topic Line'].value_counts()

city_counts = data['City'].value_counts()

country_counts = data['Country'].value_counts()


# Handle missing values
columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)

# Convert 'Timestamp' to datetime and extract date-time features
data['Timestamp'] = pd.to_datetime(data['Timestamp'])
data['Year'] = data['Timestamp'].dt.year
data['Month'] = data['Timestamp'].dt.month
data['Day'] = data['Timestamp'].dt.day
data['Hour'] = data['Timestamp'].dt.hour
data['Minute'] = data['Timestamp'].dt.minute
data['Second'] = data['Timestamp'].dt.second
data['DayOfWeek'] = data['Timestamp'].dt.dayofweek

data=data.drop('Timestamp',axis=1)
data['Age_Gender_Interact'] = data['Age'].astype(str) + '_' + data['Gender']

y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)


categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])

n_components = 250
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_processed)



X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=42)

# Define the FM-NN component
def create_fm_nn(input_dim_cat, num_embeddings):
    inputs_cat = tf.keras.layers.Input(shape=(input_dim_cat,))
    dense1 = tf.keras.layers.Dense(units=256, activation='relu')(inputs_cat)
    dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dense1)
    dense3 = tf.keras.layers.Dense(units=64, activation='relu')(dense2)
    dense4 = tf.keras.layers.Dense(units=64, activation='relu')(dense3)
    dense5 = tf.keras.layers.Dense(units=64, activation='relu')(dense4)
    nn_output = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense5)

    combined_output = nn_output

    fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
    output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)

    model_fm_nn = tf.keras.models.Model(inputs=inputs_cat, outputs=output_layer)
    return model_fm_nn


print("FM-NN")

input_dim_cat = X_train.shape[1]
num_embeddings = X_train.shape[1]
fm_nn_model = create_fm_nn(input_dim_cat, num_embeddings)
fm_nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])



# Train the FM-NN model with X_train
fm_nn_model.fit(X_train, y_train, epochs=5, batch_size=16, validation_split=0.3)
fm_nn_model_loss, fm_nn_model_accuracy = fm_nn_model.evaluate(X_test, y_test)



def create_dnn(input_dim):
    inputs = tf.keras.layers.Input(shape=(input_dim,))
    dense1 = tf.keras.layers.Dense(units=128, activation='relu')(inputs)
    # dense2 = tf.keras.layers.Dense(units=64, activation='relu')(dense1)
    # dense3 = tf.keras.layers.Dense(units=32, activation='relu')(dense2)
    # dense4 = tf.keras.layers.Dense(units=32, activation='relu')(dense1)
    output_dnn = tf.keras.layers.Dense(units=1, activation='sigmoid')(dense1)
    model_dnn = tf.keras.models.Model(inputs=inputs, outputs=output_dnn)
    return model_dnn

# Create and compile the DNN model
input_dim_dnn = X_train.shape[1]
dnn_model = create_dnn(input_dim_dnn)
dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


dnn_input = tf.keras.layers.Input(shape=(input_dim_dnn,))
fm_nn_input = tf.keras.layers.Input(shape=(input_dim_cat,))

dnn_output = create_dnn(input_dim_dnn)(dnn_input)


fm_nn_output = create_fm_nn(input_dim_cat, num_embeddings)(fm_nn_input)


combined_output = tf.keras.layers.concatenate([dnn_output, fm_nn_output])


fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
composite_output = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)


composite_model = tf.keras.models.Model(inputs=[dnn_input, fm_nn_input], outputs=composite_output)
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


composite_model.fit([X_train, X_train], y_train, epochs=6, batch_size=16)


loss, accuracy = composite_model.evaluate([X_test, X_test], y_test)
print(f"Test Loss (Composite): {loss:.4f}, Test Accuracy {accuracy:.4f}")

composite_model_predictions = composite_model.predict([X_test, X_test])
composite_model_auc = roc_auc_score(y_test, composite_model_predictions)

logloss = log_loss(y_test, composite_model_predictions)


print("FM-NN DNN")
print("AUC SCORE :",composite_model_auc)
print("LOGG LOSS :" ,logloss)

from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate ROC curve and AUC score
composite_model_predictions = composite_model.predict([X_test, X_test])
fpr, tpr, thresholds = roc_curve(y_test, composite_model_predictions)
roc_auc = roc_auc_score(y_test, composite_model_predictions)

# Plot ROC curve
plt.figure(figsize=(4, 4))
plt.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

# Create a confusion matrix
threshold = 0.5  # You can adjust the threshold based on your needs
y_pred = (composite_model_predictions > threshold).astype(int)
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()