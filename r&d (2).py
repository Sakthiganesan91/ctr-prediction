# -*- coding: utf-8 -*-
"""R&D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b8Af5kdasRP4FTXuIkWthTN1L-KpbbYw
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load your dataset
data = pd.read_csv('/content/ad_10000records.csv')  # Replace 'your_dataset.csv' with the actual filename

# Preprocess the data
X = data.drop('Clicked on Ad', axis=1)  # Features
y = data['Clicked on Ad']  # Target variable
X_encoded = pd.get_dummies(X, columns=['Ad Topic Line', 'City', 'Gender', 'Country','Timestamp'])
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Normalize the features
scaler = StandardScaler()


X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, validation_split=0.2)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from scipy import stats
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model

# Replace 'your_dataset.csv' with the actual path to your CSV file
csv_file_path = '/content/ad.csv'

# Read the CSV file into a pandas DataFrame
data = pd.read_csv(csv_file_path)

# Handling missing values as before
columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)

# Separating target variable and features
y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)

# Identify numerical columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns

# Calculate Z-scores for each numerical feature
z_scores = stats.zscore(X[numerical_columns])

# Define a threshold for outlier detection (e.g., 3 standard deviations)
threshold = 3

# Detect outliers for each feature
outliers = (z_scores > threshold) | (z_scores < -threshold)

# Replace outliers with median values
for col in numerical_columns:
    median_value = X[col].median()
    X[col][outliers[col]] = median_value

# Performing Min-Max scaling on numerical features
scaler = MinMaxScaler()
X[numerical_columns] = scaler.fit_transform(X[numerical_columns])
X['Interaction_Feature'] = X['Daily Time Spent on Site'] * X['Daily Internet Usage']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Categorical variables for embedding
categorical_vars = ['Gender', 'City']  # Add other categorical variables if needed

# Build the DNN-DCN model with L2 regularization and Dropout
embedding_layers = []
inputs = []
for var in categorical_vars:
    input_layer = Input(shape=(1,))
    embedding_layer = Embedding(input_dim=np.unique(X_train[var]).shape[0], output_dim=5,
                                embeddings_regularizer=l2(0.01))(input_layer)
    flatten_layer = Flatten()(embedding_layer)
    embedding_layers.append(flatten_layer)
    inputs.append(input_layer)

# Concatenate the embedding layers with other features
input_numeric = Input(shape=(X_train.shape[1] - len(categorical_vars),))
embedding_layers.append(input_numeric)
inputs.append(input_numeric)

concatenated = Concatenate()(embedding_layers)

dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(concatenated)
output_layer = Dense(1, activation='sigmoid')(dense_layer)

model = Model(inputs=inputs, outputs=output_layer)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV


csv_file_path = '/content/ad.csv'


data = pd.read_csv(csv_file_path)


columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)


y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)


categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])


X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


param_grid = {
    'epochs': [10, 20, 30],
    'batch_size': [32, 64, 128],
    'learning_rate': [0.001, 0.0001, 0.00001],
    'activation': ['relu', 'sigmoid'],
    'optimizer': ['adam', 'rmsprop'],
}


model = tf.keras.models.Model(inputs=[input_layer_dcn, input_layer_dnn], outputs=[output_layer])


grid_search = GridSearchCV(model, param_grid, scoring='roc_auc', cv=5)

grid_search.fit([X_train_scaled, X_train_scaled], y_train)

print(grid_search.best_params_)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss


csv_file_path = '/content/ad.csv'


data = pd.read_csv(csv_file_path)


columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)


y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)


categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])


X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
input_layer_dcn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(input_layer_dcn)
for _ in range(5):
    cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(cross_layer)
dcn_output = cross_layer
input_layer_dnn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
deep_layer = tf.keras.layers.Dense(units=256, activation='sigmoid')(input_layer_dnn)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)


dnn_output = deep_layer
combined_output = tf.keras.layers.concatenate([dcn_output, dnn_output])
fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)
composite_model = tf.keras.models.Model(inputs=[input_layer_dcn, input_layer_dnn], outputs=[output_layer])
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
composite_model.fit([X_train_scaled, X_train_scaled], y_train, epochs=10, batch_size=64, validation_split=0.2)

loss, accuracy = composite_model.evaluate([X_test_scaled, X_test_scaled], y_test)

print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")


y_pred = composite_model.predict([X_test_scaled, X_test_scaled])

auc_score = roc_auc_score(y_test, y_pred)


logloss = log_loss(y_test, y_pred)

print(f"AUC Score: {auc_score:.4f}")
print(f"Log Loss: {logloss:.4f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss


csv_file_path = '/content/ad.csv'


data = pd.read_csv(csv_file_path)


columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)


y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)


categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])




X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
input_layer_dcn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(input_layer_dcn)
for _ in range(7):
    cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(cross_layer)
dcn_output = cross_layer
input_layer_dnn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
deep_layer = tf.keras.layers.Dense(units=256, activation='sigmoid')(input_layer_dnn)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)
dnn_output = deep_layer
combined_output = tf.keras.layers.concatenate([dcn_output, dnn_output])
fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)
composite_model = tf.keras.models.Model(inputs=[input_layer_dcn, input_layer_dnn], outputs=[output_layer])
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
composite_model.fit([X_train_scaled, X_train_scaled], y_train, epochs=5, batch_size=32, validation_split=0.2)

loss, accuracy = composite_model.evaluate([X_test_scaled, X_test_scaled], y_test)

print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")


y_pred = composite_model.predict([X_test_scaled, X_test_scaled])


auc_score = roc_auc_score(y_test, y_pred)


logloss = log_loss(y_test, y_pred)

print(f"AUC Score: {auc_score:.4f}")
print(f"Log Loss: {logloss:.4f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from keras.regularizers import l2
from keras.models import Model
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import roc_auc_score, log_loss


csv_file_path = '/content/ad.csv'


data = pd.read_csv(csv_file_path)

data.head(10)

import pandas as pd
ad_counts = data['Ad Topic Line'].value_counts()
print(ad_counts.head(10))
city_counts = data['City'].value_counts()
print(city_counts.head(10))
country_counts = data['Country'].value_counts()
print(country_counts.head(10))

columns_with_missing = data.columns[data.isnull().any()]
for column in columns_with_missing:
    if data[column].dtype != 'object':
        median_value = data[column].median()
        data[column].fillna(median_value, inplace=True)
    else:
        mode_value = data[column].mode()[0]
        data[column].fillna(mode_value, inplace=True)

#Feature Engineering
data['Timestamp'] = pd.to_datetime(data['Timestamp'])

data['Year'] = data['Timestamp'].dt.year
data['Month'] = data['Timestamp'].dt.month
data['Day'] = data['Timestamp'].dt.day
data['Hour'] = data['Timestamp'].dt.hour
data['Minute'] = data['Timestamp'].dt.minute
data['Second'] = data['Timestamp'].dt.second
data['DayOfWeek'] = data['Timestamp'].dt.dayofweek

data['Age_Gender_Interact'] = data['Age'].astype(str) + '_' + data['Gender']

data=data.drop("Timestamp",axis=1)

data.head(10)

y = data['Clicked on Ad']
X = data.drop('Clicked on Ad', axis=1)

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, log_loss
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt



categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns], X_categorical], axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])

n_components = 250
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_processed)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)


input_layer_dcn = tf.keras.layers.Input(shape=(X_train.shape[1],))
cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(input_layer_dcn)
for _ in range(9):
    cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(cross_layer)
dcn_output = cross_layer

input_layer_dnn = tf.keras.layers.Input(shape=(X_train.shape[1],))
deep_layer = tf.keras.layers.Dense(units=256, activation='sigmoid')(input_layer_dnn)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)

dnn_output = deep_layer


combined_output = tf.keras.layers.concatenate([dcn_output, dnn_output])
fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)


composite_model = tf.keras.models.Model(inputs=[input_layer_dcn, input_layer_dnn], outputs=[output_layer])
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


composite_model.fit([X_train, X_train], y_train, epochs=5, batch_size=32, validation_split=0.2)


loss, accuracy = composite_model.evaluate([X_test, X_test], y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")


y_pred = composite_model.predict([X_test, X_test])
auc_score = roc_auc_score(y_test, y_pred)
logloss = log_loss(y_test, y_pred)

print(f"AUC Score: {auc_score:.4f}")
print(f"Log Loss: {logloss:.4f}")
from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) DCN-DNN MODEL')
plt.legend(loc='lower right')
plt.show()

print(X_pca)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict binary class labels (0 or 1) based on your model's output probabilities
y_pred_binary = (y_pred > 0.6).astype(int)

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_binary)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix DNN-DCN')
plt.show()

categorical_columns = X.select_dtypes(include=['object']).columns
numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns
X_categorical = pd.get_dummies(X[categorical_columns], drop_first=True)
X_processed = pd.concat([X[numerical_columns],X_categorical] ,axis=1)

scaler = MinMaxScaler()
X_processed[numerical_columns] = scaler.fit_transform(X_processed[numerical_columns])

X_processed

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
input_layer_dcn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(input_layer_dcn)
for _ in range(9):
    cross_layer = tf.keras.layers.Dense(units=32, activation='relu')(cross_layer)
dcn_output = cross_layer
input_layer_dnn = tf.keras.layers.Input(shape=(X_train_scaled.shape[1],))
deep_layer = tf.keras.layers.Dense(units=256, activation='sigmoid')(input_layer_dnn)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)
deep_layer = tf.keras.layers.Dense(units=64, activation='sigmoid')(deep_layer)

dnn_output = deep_layer
combined_output = tf.keras.layers.concatenate([dcn_output, dnn_output])
fusion_layer = tf.keras.layers.Dense(units=64, activation='relu')(combined_output)
output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion_layer)
composite_model = tf.keras.models.Model(inputs=[input_layer_dcn, input_layer_dnn], outputs=[output_layer])
composite_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
composite_model.fit([X_train_scaled, X_train_scaled], y_train, epochs=5, batch_size=32, validation_split=0.2)
loss, accuracy = composite_model.evaluate([X_test_scaled, X_test_scaled], y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")
y_pred = composite_model.predict([X_test_scaled, X_test_scaled])
auc_score = roc_auc_score(y_test, y_pred)
logloss = log_loss(y_test, y_pred)

print(f"AUC Score: {auc_score:.4f}")
print(f"Log Loss: {logloss:.4f}")

subset_size = 50
plt.figure(figsize=(10, 6))
plt.scatter(range(subset_size), y_test[:subset_size], marker='o', label='Actual Labels', color='blue')
plt.scatter(range(subset_size), y_pred[:subset_size], marker='x', label='Predicted Labels', color='red')
plt.xlabel('Sample Index')
plt.ylabel('Labels')
plt.title('Actual vs. Predicted Labels')
plt.legend()
plt.show()